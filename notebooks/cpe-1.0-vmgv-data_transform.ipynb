{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación de los datos en bruto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modulos a emplear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerias a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Funciones importadas como modulos\n",
    "notebook_dir = os.getcwd()\n",
    "scripts_dir = os.path.join(notebook_dir, \"..\", \"src\")\n",
    "sys.path.append(scripts_dir)\n",
    "from data_transform.load_csv import load_csv\n",
    "from data_transform.map_cimas_to_depth_multiple import map_cimas_create_columns\n",
    "\n",
    "# Codigo para imprimir mas de una salida de la misma celda\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos la informacion de los registros de pozo y cimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas de los registros de pozo y cimas \n",
    "logs_file_name = \"logs_pijije_rhob_nphi_dtco_syn.csv\"\n",
    "tops_file_name = \"tops_pijije_rhob_nphi_dtco_syn.csv\"\n",
    "path_well_logs = os.path.join(notebook_dir, \"..\", \"data\", \"raw\", logs_file_name)\n",
    "path_well_tops = os.path.join(notebook_dir, \"..\", \"data\", \"raw\", tops_file_name)\n",
    "\n",
    "# Cargamos la informacion de los pozos\n",
    "well_logs_df = load_csv(path_well_logs)\n",
    "\n",
    "# Cambiamos el tipo de de columnas a float\n",
    "columnas_convertir = [\"md\", \"dtco\",\"dtsm\", \"gr\", \"rp\"]\n",
    "well_logs_df[columnas_convertir] = well_logs_df[columnas_convertir].astype(float)\n",
    "\n",
    "# Verificamos que no exista ninguna valor negativo en md\n",
    "well_logs_df = well_logs_df[well_logs_df[\"md\"] >= 0]\n",
    "\n",
    "# Cargamos la informacion de las cimas\n",
    "well_tops_df = load_csv(path_well_tops)\n",
    "\n",
    "# Cambiamos el tipo de columna a float\n",
    "well_tops_df[\"md\"] = well_tops_df[\"md\"].astype(float)\n",
    "\n",
    "# Ordena el DataFrame de cimas por 'MD' para asegurarte de que esté en el orden correcto\n",
    "well_tops_df = well_tops_df.sort_values(by='md')\n",
    "\n",
    "# Borramos la cima \"PT\"\n",
    "well_tops_df = well_tops_df[well_tops_df[\"surface\"] != \"PT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las cimas con los pozos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos las cimas con los registros de pozo \n",
    "pijije_df = map_cimas_create_columns(well_tops_df, well_logs_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos el campo al que pertenecen los pozos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el campo al que pertenecen los pozos\n",
    "field = \"Caparroso-Pijije-Escuintle\"\n",
    "pijije_df[\"field\"] = field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el archivo SQL para el llenado de la tabla wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         wellname      min_md       max_md                       field\n",
      "0    Caparroso-71  499.903015  6120.719727  Caparroso-Pijije-Escuintle\n",
      "1    Caparroso-75   60.000000  5614.065600  Caparroso-Pijije-Escuintle\n",
      "2      Pijije-101    0.000000  5233.111200  Caparroso-Pijije-Escuintle\n",
      "3      Pijije-111    0.000800  5851.856000  Caparroso-Pijije-Escuintle\n",
      "4      Pijije-112    0.062000  5149.962891  Caparroso-Pijije-Escuintle\n",
      "5      Pijije-117    0.000000  5636.056641  Caparroso-Pijije-Escuintle\n",
      "6      Pijije-125   40.081200  5271.058800  Caparroso-Pijije-Escuintle\n",
      "7      Pijije-127   46.024799  5414.924316  Caparroso-Pijije-Escuintle\n",
      "8       Pijije-1A    0.000000  6030.925200  Caparroso-Pijije-Escuintle\n",
      "9       Pijije-21    0.000000  5332.018800  Caparroso-Pijije-Escuintle\n",
      "10      Pijije-25    0.076400  5900.547200  Caparroso-Pijije-Escuintle\n",
      "11  Pijije-25REE2    0.000000  6049.975200  Caparroso-Pijije-Escuintle\n",
      "12      Pijije-27   27.600000  5379.900000  Caparroso-Pijije-Escuintle\n",
      "13   Pijije-31REE    0.000000  4874.971200  Caparroso-Pijije-Escuintle\n",
      "14      Pijije-33   49.072800  5068.062000  Caparroso-Pijije-Escuintle\n",
      "15      Pijije-34    0.000000  4799.990400  Caparroso-Pijije-Escuintle\n",
      "16      Pijije-41  500.000000  6334.024400  Caparroso-Pijije-Escuintle\n",
      "17      Pijije-42    0.000000  4703.978400  Caparroso-Pijije-Escuintle\n"
     ]
    }
   ],
   "source": [
    "# Agrupamos por wellname y calculamos el mínimo y máximo de md, y el campo field\n",
    "grouped_df = pijije_df.groupby('wellname').agg({'md': ['min', 'max'], 'field': 'first'}).reset_index()\n",
    "\n",
    "# Renombramos las columnas para mayor claridad\n",
    "grouped_df.columns = ['wellname', 'min_md', 'max_md', 'field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo insert_queries.sql creado exitosamente en d:\\Usuarios\\574905\\Documents\\Python\\CPE\\Sinteticos\\CPE\\notebooks\\..\\sql\\insert_queries.sql.\n"
     ]
    }
   ],
   "source": [
    "# Crear una lista para almacenar las consultas SQL\n",
    "sql_queries = []\n",
    "\n",
    "# Iterar sobre las filas del DataFrame\n",
    "for index, row in grouped_df.iterrows():\n",
    "    wellname = row['wellname']\n",
    "    min_md = row['min_md']\n",
    "    max_md = row['max_md']\n",
    "    field = row['field']\n",
    "\n",
    "    # Crear la cadena SQL para la entrada\n",
    "    sql_query = f\"INSERT INTO wells (well, field, top_md, bottom_md) VALUES ('{wellname}', '{field}', {max_md}, {min_md});\"\n",
    "    \n",
    "    # Agregar la consulta SQL a la lista\n",
    "    sql_queries.append(sql_query)\n",
    "\n",
    "# Guardar las consultas SQL en un archivo .sql\n",
    "sql_file_name = \"insert_queries.sql\"\n",
    "path = os.path.join(notebook_dir, \"..\", \"sql\", sql_file_name)\n",
    "\n",
    "with open(path, \"w\") as sql_file:\n",
    "    for query in sql_queries:\n",
    "        sql_file.write(query + \"\\n\")\n",
    "\n",
    "print(f\"Archivo {sql_file_name} creado exitosamente en {path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el archivo SQL para el llenado de la tabla logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos el DataSet con la informacion procesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el DataSet pijije_procesed en formato csv\n",
    "processed_file_name = \"data_pijije_rhob_nphi_dtco_syn.csv\"\n",
    "path_to_save = os.path.join(notebook_dir, \"..\", \"data\", \"processed\", processed_file_name)\n",
    "pijije_df.to_csv(path_to_save, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
